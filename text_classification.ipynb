{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n","metadata":{"id":"V6RsVZAmke8s","execution":{"iopub.status.busy":"2023-10-27T18:11:04.683475Z","iopub.execute_input":"2023-10-27T18:11:04.683843Z","iopub.status.idle":"2023-10-27T18:11:05.064636Z","shell.execute_reply.started":"2023-10-27T18:11:04.683812Z","shell.execute_reply":"2023-10-27T18:11:05.063697Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2SlMnicoP11","outputId":"a4f7c32d-7515-4f0a-d33f-738fbb26c567","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/newsdata/dataset_fakenews.csv')","metadata":{"id":"MOol9SEwodym","execution":{"iopub.status.busy":"2023-10-27T18:11:05.066448Z","iopub.execute_input":"2023-10-27T18:11:05.066911Z","iopub.status.idle":"2023-10-27T18:11:05.150166Z","shell.execute_reply.started":"2023-10-27T18:11:05.066875Z","shell.execute_reply":"2023-10-27T18:11:05.149129Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bx_m48bMpbDo","outputId":"2c0a2ba1-aa68-41c2-c550-9d0f6c31304a","execution":{"iopub.status.busy":"2023-10-27T18:11:05.151893Z","iopub.execute_input":"2023-10-27T18:11:05.152275Z","iopub.status.idle":"2023-10-27T18:11:18.417870Z","shell.execute_reply.started":"2023-10-27T18:11:05.152243Z","shell.execute_reply":"2023-10-27T18:11:18.416859Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as functional\nimport matplotlib.pyplot as plt\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nimport gc\nfrom transformers import BertModel\nfrom sklearn.metrics import roc_auc_score,f1_score\nimport time\nimport datetime","metadata":{"id":"MeUAHWcSpos9","execution":{"iopub.status.busy":"2023-10-27T18:11:18.420403Z","iopub.execute_input":"2023-10-27T18:11:18.420702Z","iopub.status.idle":"2023-10-27T18:11:32.216314Z","shell.execute_reply.started":"2023-10-27T18:11:18.420672Z","shell.execute_reply":"2023-10-27T18:11:32.215406Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(0)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda' if use_cuda else 'cpu')\nif use_cuda:\n    torch.cuda.manual_seed(0)\n\nprint(\"Using GPU: {}\".format(use_cuda))","metadata":{"execution":{"iopub.status.busy":"2023-10-27T18:11:32.217611Z","iopub.execute_input":"2023-10-27T18:11:32.217937Z","iopub.status.idle":"2023-10-27T18:11:32.290767Z","shell.execute_reply.started":"2023-10-27T18:11:32.217910Z","shell.execute_reply":"2023-10-27T18:11:32.289772Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Using GPU: True\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ds_0JrLlpvLb","outputId":"075addf5-a858-4e0b-dbeb-28021de543b5","execution":{"iopub.status.busy":"2023-10-27T18:11:32.292019Z","iopub.execute_input":"2023-10-27T18:11:32.292324Z","iopub.status.idle":"2023-10-27T18:11:32.861202Z","shell.execute_reply.started":"2023-10-27T18:11:32.292299Z","shell.execute_reply":"2023-10-27T18:11:32.859968Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Loading BERT tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad1bb54befc4cce83c38d3cd440c3ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8004438702e14057b5530aafc0322736"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ccfd19a10034c5ca744f3ad75f5c034"}},"metadata":{}}]},{"cell_type":"code","source":"tweets = data.Tweet.values\nlabels = data.label.values","metadata":{"id":"6xpdiy2Kp1Fl","execution":{"iopub.status.busy":"2023-10-27T18:11:32.862450Z","iopub.execute_input":"2023-10-27T18:11:32.862793Z","iopub.status.idle":"2023-10-27T18:11:32.872812Z","shell.execute_reply.started":"2023-10-27T18:11:32.862752Z","shell.execute_reply":"2023-10-27T18:11:32.871710Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import re\nfrom string import punctuation\ndef preprocess(data):\n    #remove url and hashtag\n    for i in range(data.shape[0]):\n        text=data[i].lower()\n        text1=''.join([word+\" \" for word in text.split()])\n        data[i]=text1\n    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    hashtag_regex = '#[\\w\\-]+'\n    space_pattern = '\\s+'\n\n    for i in range(data.shape[0]):\n        text_string = data[i]\n        parsed_text = re.sub(hashtag_regex, '', text_string)\n        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n        parsed_text = re.sub(mention_regex, '', parsed_text)\n        #remove punctuation\n        parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n        parsed_text = re.sub(space_pattern, ' ', parsed_text)\n        data[i] = parsed_text\n    return data\ntweets = preprocess(tweets)\nprint(tweets)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"URUk2GFep8g-","outputId":"7a37f24f-2d42-4ba6-dd4b-f67e3ab38549","execution":{"iopub.status.busy":"2023-10-27T18:11:32.873981Z","iopub.execute_input":"2023-10-27T18:11:32.874280Z","iopub.status.idle":"2023-10-27T18:11:33.249553Z","shell.execute_reply.started":"2023-10-27T18:11:32.874256Z","shell.execute_reply":"2023-10-27T18:11:33.248555Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['the cdc currently reports 99031 deaths in general the discrepancies in death counts between different sources are small and explicable the death toll stands at roughly 100000 people today '\n 'states reported 1121 deaths a small rise from last tuesday southern states reported 640 of those deaths '\n 'politically correct woman almost uses pandemic as excuse not to reuse plastic bag '\n ...\n 'breaking news according to documents released to the press was connected to an earlier robbery url '\n 'ebola vaccines url '\n 'concerned airport passenger suits up in homemade hazmat before flight url url ']\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\nfor tweet in tweets:\n    encoded_dict = tokenizer.encode_plus(\n                        tweet,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 512,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n\n    # Add the encoded sentence to the list.\n    input_ids.append(encoded_dict['input_ids'])\n\n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', tweets[0])\nprint('Token IDs:', input_ids[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ap3CaYJOqDMc","outputId":"702d5352-b190-4176-9596-96ced569d92c","execution":{"iopub.status.busy":"2023-10-27T18:11:33.251009Z","iopub.execute_input":"2023-10-27T18:11:33.251314Z","iopub.status.idle":"2023-10-27T18:11:50.379227Z","shell.execute_reply.started":"2023-10-27T18:11:33.251289Z","shell.execute_reply":"2023-10-27T18:11:50.378182Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Original:  the cdc currently reports 99031 deaths in general the discrepancies in death counts between different sources are small and explicable the death toll stands at roughly 100000 people today \nToken IDs: tensor([  101,  1996, 26629,  2747,  4311,  5585,  2692, 21486,  6677,  1999,\n         2236,  1996,  5860,  2890,  9739,  9243,  1999,  2331,  9294,  2090,\n         2367,  4216,  2024,  2235,  1998,  4654, 24759,  5555,  3468,  1996,\n         2331,  9565,  4832,  2012,  5560,  6694,  8889,  2111,  2651,   102,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0])\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\n# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 90-10 train-validation split.\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size],generator=torch.Generator().manual_seed(42))\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oJKcEGPqaNL","outputId":"d933718b-6b1f-43a9-c945-5c069251ce1c","execution":{"iopub.status.busy":"2023-10-27T18:11:50.383640Z","iopub.execute_input":"2023-10-27T18:11:50.384057Z","iopub.status.idle":"2023-10-27T18:11:50.396006Z","shell.execute_reply.started":"2023-10-27T18:11:50.384024Z","shell.execute_reply":"2023-10-27T18:11:50.395106Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"9,045 training samples\n1,005 validation samples\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 16\n\ntrain_dataloader = DataLoader(\n            train_dataset,\n            shuffle = True,\n            batch_size = batch_size\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset,\n            shuffle = False,\n            batch_size = batch_size\n        )","metadata":{"id":"agHNy9b_qcj_","execution":{"iopub.status.busy":"2023-10-27T18:11:50.397018Z","iopub.execute_input":"2023-10-27T18:11:50.397346Z","iopub.status.idle":"2023-10-27T18:11:50.403709Z","shell.execute_reply.started":"2023-10-27T18:11:50.397301Z","shell.execute_reply":"2023-10-27T18:11:50.402693Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"id":"vf1MCWREqlvw","execution":{"iopub.status.busy":"2023-10-27T18:11:50.404756Z","iopub.execute_input":"2023-10-27T18:11:50.405022Z","iopub.status.idle":"2023-10-27T18:11:50.412768Z","shell.execute_reply.started":"2023-10-27T18:11:50.405000Z","shell.execute_reply":"2023-10-27T18:11:50.411830Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-Nsj46hqykR","outputId":"41e86704-965f-480a-baf6-694d5f6477a9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 2,\n    output_attentions = False,\n    output_hidden_states = False,\n)\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2Zr2vqHqo2-","outputId":"78869e31-c6c5-4922-91b5-e45f33d813c9","execution":{"iopub.status.busy":"2023-10-27T18:11:50.413900Z","iopub.execute_input":"2023-10-27T18:11:50.414175Z","iopub.status.idle":"2023-10-27T18:11:59.492942Z","shell.execute_reply.started":"2023-10-27T18:11:50.414152Z","shell.execute_reply":"2023-10-27T18:11:59.492079Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a976b0e90fa401d8219c44504a8c35c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 5e-5,\n                  eps = 1e-8\n                )\nepochs = 1\ncriterion = nn.CrossEntropyLoss()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JIzz6u2gq2yp","outputId":"a1072bbe-380a-4d64-8840-8f5e385f1612","execution":{"iopub.status.busy":"2023-10-27T18:11:59.494204Z","iopub.execute_input":"2023-10-27T18:11:59.494593Z","iopub.status.idle":"2023-10-27T18:11:59.505736Z","shell.execute_reply.started":"2023-10-27T18:11:59.494564Z","shell.execute_reply":"2023-10-27T18:11:59.504778Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport numpy as np\n\nseed_val = 42\n\nrandom.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ntraining_stats = []\ntotal_t0 = time.time()\nbest_accuracy = 0\nfor epoch_i in range(0, epochs):\n    #Training\n    print(\"\")\n    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    t0 = time.time()\n    total_train_loss = 0\n    total_train_accuracy = 0\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n\n        input_ids = batch[0].to(device)\n        input_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n\n        model.zero_grad()\n        out = model(input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels)\n        loss = out[0]\n        logits = out[1]\n\n        total_train_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        pred = torch.argmax(logits, dim = 1)\n        total_train_accuracy +=  torch.sum(pred == labels).item()\n\n    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n    print(\"  Training loss: {}\".format(avg_train_loss))\n\n\n    # Validation\n    print(\"\")\n    print(\"Validation...\")\n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    y_true = []\n    y_pred = []\n\n    for batch in validation_dataloader:\n        input_ids = batch[0].to(device)\n        input_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n\n        with torch.no_grad():\n            out = model(input_ids, token_type_ids=None, attention_mask=input_mask,labels=labels)\n            loss = out[0]\n            logits = out[1]\n\n        total_eval_loss += loss.item()\n        pred = torch.argmax(logits, dim = 1)\n        total_eval_accuracy += torch.sum(pred == labels).item()\n        y_true.append(labels.flatten())\n        y_pred.append(pred.flatten())\n\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n    print(\"  Validation loss: {}\".format(avg_val_loss))\n    training_time = format_time(time.time() - t0)\n    print()\n\n    y_true = torch.cat(y_true).tolist()\n    y_pred = torch.cat(y_pred).tolist()\n    print(\"This epoch took: {:}\".format(training_time))\n    print('roc_auc score: ', roc_auc_score(y_true,y_pred))\n    print('F1 score:',f1_score(y_true, y_pred))\n    print()\n\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Train Accur.': avg_train_accuracy,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n        }\n    )\n    print()\n\n    if avg_val_accuracy > best_accuracy:\n        best_accuracy = avg_val_accuracy\n        best_model = model\n\nprint()\nprint(\"=\"*10)\nprint(\"Summary\")\nprint(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwCngKCVq-6U","outputId":"0fe7829c-4e43-4755-9986-6dd0187f567b","execution":{"iopub.status.busy":"2023-10-27T18:11:59.507343Z","iopub.execute_input":"2023-10-27T18:11:59.507708Z","iopub.status.idle":"2023-10-27T20:31:46.323565Z","shell.execute_reply.started":"2023-10-27T18:11:59.507669Z","shell.execute_reply":"2023-10-27T20:31:46.322555Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\nEpoch 1 / 10\nTraining...\n  Accuracy: 0.8725262576008844\n  Training loss: 0.018541454331822707\n\nValidation...\n  Accuracy: 0.9014925373134328\n  Validation loss: 0.017291091498339652\n\nThis epoch took: 0:13:55\nroc_auc score:  0.9005151575193185\nF1 score: 0.8950159066808059\n\n\n\nEpoch 2 / 10\nTraining...\n  Accuracy: 0.9551133222775013\n  Training loss: 0.008771878010030905\n\nValidation...\n  Accuracy: 0.9233830845771144\n  Validation loss: 0.020284295202214364\n\nThis epoch took: 0:13:58\nroc_auc score:  0.9235585496334456\nF1 score: 0.9221435793731041\n\n\n\nEpoch 3 / 10\nTraining...\n  Accuracy: 0.9828634604754007\n  Training loss: 0.004135872849818846\n\nValidation...\n  Accuracy: 0.9203980099502488\n  Validation loss: 0.025912592157782913\n\nThis epoch took: 0:13:59\nroc_auc score:  0.919407568852784\nF1 score: 0.9150743099787685\n\n\n\nEpoch 4 / 10\nTraining...\n  Accuracy: 0.9931453841901603\n  Training loss: 0.0020053668732077823\n\nValidation...\n  Accuracy: 0.9243781094527364\n  Validation loss: 0.02809630734969072\n\nThis epoch took: 0:13:59\nroc_auc score:  0.9241331484049929\nF1 score: 0.9218106995884773\n\n\n\nEpoch 5 / 10\nTraining...\n  Accuracy: 0.996130458817026\n  Training loss: 0.001170703309229911\n\nValidation...\n  Accuracy: 0.9283582089552239\n  Validation loss: 0.028450027074006304\n\nThis epoch took: 0:14:00\nroc_auc score:  0.9282147810580543\nF1 score: 0.9262295081967213\n\n\n\nEpoch 6 / 10\nTraining...\n  Accuracy: 0.9952459922609176\n  Training loss: 0.0012856285650155767\n\nValidation...\n  Accuracy: 0.9273631840796019\n  Validation loss: 0.030027473672661342\n\nThis epoch took: 0:13:59\nroc_auc score:  0.9269962353873589\nF1 score: 0.9245087900723887\n\n\n\nEpoch 7 / 10\nTraining...\n  Accuracy: 0.9957987838584853\n  Training loss: 0.0014406507969605496\n\nValidation...\n  Accuracy: 0.9273631840796019\n  Validation loss: 0.03210813808282409\n\nThis epoch took: 0:13:59\nroc_auc score:  0.9274420447790768\nF1 score: 0.9258883248730965\n\n\n\nEpoch 8 / 10\nTraining...\n  Accuracy: 0.993698175787728\n  Training loss: 0.001995220427713711\n\nValidation...\n  Accuracy: 0.9213930348258706\n  Validation loss: 0.03325393912551834\n\nThis epoch took: 0:13:59\nroc_auc score:  0.9223102833366357\nF1 score: 0.9224730127576055\n\n\n\nEpoch 9 / 10\nTraining...\n  Accuracy: 0.9969043670536207\n  Training loss: 0.0010960106487428113\n\nValidation...\n  Accuracy: 0.9293532338308458\n  Validation loss: 0.034389517691377124\n\nThis epoch took: 0:13:59\nroc_auc score:  0.9292351892213195\nF1 score: 0.9273285568065507\n\n\n\nEpoch 10 / 10\nTraining...\n  Accuracy: 0.9974571586511884\n  Training loss: 0.0009672365767890629\n\nValidation...\n  Accuracy: 0.9273631840796019\n  Validation loss: 0.033509461831913064\n\nThis epoch took: 0:14:00\nroc_auc score:  0.9272934416485039\nF1 score: 0.9254341164453523\n\n\n\n==========\nSummary\nTotal time 2:19:47 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"# v=\"model_5\"\n# model.save(f\"/kaggle/working/{v}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T20:31:46.324913Z","iopub.execute_input":"2023-10-27T20:31:46.325685Z","iopub.status.idle":"2023-10-27T20:31:46.329867Z","shell.execute_reply.started":"2023-10-27T20:31:46.325647Z","shell.execute_reply":"2023-10-27T20:31:46.328867Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"PATH1 = \"/kaggle/working/text_classification_model2.pt\"\ntorch.save(model, PATH1)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T20:34:45.588492Z","iopub.execute_input":"2023-10-27T20:34:45.589511Z","iopub.status.idle":"2023-10-27T20:34:46.487305Z","shell.execute_reply.started":"2023-10-27T20:34:45.589457Z","shell.execute_reply":"2023-10-27T20:34:46.486102Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(data):\n    text=data.lower()\n    text1=''.join([word+\" \" for word in text.split()])\n    data=text1\n    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n    '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    hashtag_regex = '#[\\w\\-]+'\n    space_pattern = '\\s+'\n\n\n    text_string = data\n    parsed_text = re.sub(hashtag_regex, '', text_string)\n    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n    parsed_text = re.sub(mention_regex, '', parsed_text)\n    #remove punctuation\n    parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n    parsed_text = re.sub(space_pattern, ' ', parsed_text)\n    data = parsed_text\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-10-27T20:31:46.971494Z","iopub.execute_input":"2023-10-27T20:31:46.971802Z","iopub.status.idle":"2023-10-27T20:31:46.978817Z","shell.execute_reply.started":"2023-10-27T20:31:46.971776Z","shell.execute_reply":"2023-10-27T20:31:46.977981Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Define the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Load the pre-trained model architecture\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, output_attentions=False, output_hidden_states=False)\n\n# Move the model to the appropriate device\ndevice = \"cpu\"\nmodel.to(device)\n\n# Create a function to predict for a single text input\ndef predict_single_text(text):\n    # Preprocess the text input\n    text = preprocess_text(text)  # Replace with your preprocessing function\n\n    # Tokenize and convert the input to the required format\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\n    # Make the prediction\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    probabilities = torch.softmax(logits, dim=1)\n    predicted_class = torch.argmax(probabilities, dim=1).item()\n\n    return predicted_class, probabilities\n\n# Example usage\ntext_input = \"abc reportedly gave darren wilson 6-figures for an interview. if true, they essentially paid the bounty for killing a black child. #ferguson\"\npredicted_class, class_probabilities = predict_single_text(text_input)\nprint(\"Predicted Class:\", predicted_class)\nprint(\"Class Probabilities:\", class_probabilities)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T20:31:46.979969Z","iopub.execute_input":"2023-10-27T20:31:46.980238Z","iopub.status.idle":"2023-10-27T20:31:48.592129Z","shell.execute_reply.started":"2023-10-27T20:31:46.980216Z","shell.execute_reply":"2023-10-27T20:31:48.591106Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Predicted Class: 0\nClass Probabilities: tensor([[0.5691, 0.4309]])\n","output_type":"stream"}]},{"cell_type":"code","source":"text_input = \"a good relationship: â˜‘ calls you to just say hi â˜‘ wants to see you â˜‘ brings you bk\"\npredicted_class, class_probabilities = predict_single_text(text_input)\nprint(\"Predicted Class:\", predicted_class)\nprint(\"Class Probabilities:\", class_probabilities)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T20:31:48.593513Z","iopub.execute_input":"2023-10-27T20:31:48.593896Z","iopub.status.idle":"2023-10-27T20:31:48.667468Z","shell.execute_reply.started":"2023-10-27T20:31:48.593860Z","shell.execute_reply":"2023-10-27T20:31:48.666531Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Predicted Class: 0\nClass Probabilities: tensor([[0.6160, 0.3840]])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport re\nfrom string import punctuation\n\n# Define the device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Load the trained model\nmodel_path = 'path_to_your_saved_model.pth'  # Replace with the actual path to your saved model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.to(device)\nmodel.eval()  # Set the model to evaluation mode\n\n# Example raw tweet (not preprocessed)\nraw_tweet = \"???Clearly, the Obama administration did not leave any kind of game plan for something like this.??�\"\n\n# Preprocessing function to replicate training preprocessing steps\ndef preprocess_text(data):\n    text = data.lower()\n    text1 = ''.join([word + \" \" for word in text.split()])\n    data = text1\n    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n    '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    hashtag_regex = '#[\\w\\-]+'\n    space_pattern = '\\s+'\n\n    text_string = data\n    parsed_text = re.sub(hashtag_regex, '', text_string)\n    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n    parsed_text = re.sub(mention_regex, '', parsed_text)\n    # Remove punctuation\n    parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n    parsed_text = re.sub(space_pattern, ' ', parsed_text)\n    data = parsed_text\n    return data\n\n# Preprocess the raw tweet using your provided function\npreprocessed_tweet = preprocess_text(raw_tweet)\n\n# Tokenize the preprocessed tweet\nencoded_dict = tokenizer.encode_plus(\n                    preprocessed_tweet, add_special_tokens=True, max_length=512,\n                    pad_to_max_length=True, return_attention_mask=True,\n                    return_tensors='pt'\n               )\n\n# Extract the encoded input IDs and attention mask\ninput_ids = encoded_dict['input_ids'].to(device)\nattention_mask = encoded_dict['attention_mask'].to(device)\n\n# Make predictions\nwith torch.no_grad():\n    outputs = model(input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n    predicted_class = torch.argmax(logits, dim=1).item()\n\n# Print the predicted class (0 for fake news, 1 for real news)\nif predicted_class == 1:\n    print(\"Predicted: Fake News\")\nelse:\n    print(\"Predicted: Real News\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport re\nfrom string import punctuation\n\n# Define the device (CPU or GPU)\ndevice = torch.device(\"cpu\")\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Replace 'path_to_your_saved_model.pth' with the actual path to your saved model checkpoint\nmodel_path = '/kaggle/input/test-analysis-model/text_classification_model2.pt'\n\n# Load the entire model\nloaded_model = torch.load(model_path, map_location=device)\n\n# Set the model to evaluation mode\nloaded_model.eval()\n\n# Example raw tweet (not preprocessed)\nraw_tweet = \"\"\n# Preprocessing function to replicate training preprocessing steps\ndef preprocess_text(data):\n    text = data.lower()\n    text1 = ''.join([word + \" \" for word in text.split()])\n    data = text1\n    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n    '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    hashtag_regex = '#[\\w\\-]+'\n    space_pattern = '\\s+'\n\n    text_string = data\n    parsed_text = re.sub(hashtag_regex, '', text_string)\n    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n    parsed_text = re.sub(mention_regex, '', parsed_text)\n    # Remove punctuation\n    parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n    parsed_text = re.sub(space_pattern, ' ', parsed_text)\n    data = parsed_text\n    return data\n\n# Preprocess the raw tweet using your provided function\npreprocessed_tweet = preprocess_text(raw_tweet)\n\n# Tokenize the preprocessed tweet\nencoded_dict = tokenizer.encode_plus(\n                    preprocessed_tweet, add_special_tokens=True, max_length=512,\n                    pad_to_max_length=True, return_attention_mask=True,\n                    return_tensors='pt'\n               )\n\n# Extract the encoded input IDs and attention mask\ninput_ids = encoded_dict['input_ids'].to(device)\nattention_mask = encoded_dict['attention_mask'].to(device)\n\nwith torch.no_grad():\n    outputs = loaded_model(input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n    predicted_class = torch.argmax(logits, dim=1).item()\n\n# Print the predicted class (1 for fake news, 0 for real news)\nif predicted_class == 1:\n    print(\"Predicted: Fake News\")\nelse:\n    print(\"Predicted: Real News\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:27:31.145616Z","iopub.execute_input":"2023-10-28T06:27:31.145997Z","iopub.status.idle":"2023-10-28T06:27:31.975173Z","shell.execute_reply.started":"2023-10-28T06:27:31.145964Z","shell.execute_reply":"2023-10-28T06:27:31.974129Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Predicted: Fake News\n","output_type":"stream"}]}]}